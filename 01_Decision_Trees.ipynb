{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Supervised Learning with Decision Trees</h1></center>\n",
    "\n",
    "Now that we've built a classifier by hand, we have a general intuition behind the first Machine Learning algorithm we'll be using--[Decision Tree Classifiers](http://scikit-learn.org/stable/modules/tree.html#tree).  **_Decision Trees_** are considered one of the more mature, traditional algorithms in predictive analytics.  Although they can also be used for regression, you'll most likely see them used for classification problems.  \n",
    "\n",
    "Before we begin using one in code, please take a few minutes to check out [this great tutorial](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) about how decision tress work.  \n",
    "<br>\n",
    "**_No, seriously.  Stop what you are doing and follow that link.  It contains one of the coolest visualizations of machine learning you'll ever see._**\n",
    "\n",
    "\n",
    "<center><h3>A Brief Primer on Decision Trees</h3></center>\n",
    "\n",
    "As explained in the SciKit-Learn user guide for this model, Decision Trees are \"a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\" Let's explore what this means in plain English:\n",
    "\n",
    "* Decision Trees are **_non-parametric_**.  This means that they make no underlying assumptions about the distribution of the dataset that they are working with.  Non-parametric models are not necessarily better or worse than parametric models, just better suited for different use cases.  \n",
    "\n",
    "* Decision Trees can be used for **_Classification_** or **_Regression_**.  \n",
    "\n",
    "* Decision Trees make predictions by combining a series of simple decisions based on what it sees in the training data. These decisions are nested, to form a tree. In the last exercise, when you wrote your classifier by hand, you probably did the same thing. For instance, it's not uncommon for students to explain their handmade model by saying somnething like \"If the person I'm predicting for is a woman, I classify them as 'survived'.  If they're a man, I look at their ticket class.  If their ticket is 1st class, I predict they survived.  Otherwise, I predict they died\".  These nested sets of decisions can be represented as a tree, hence the name--Decision Tree!\n",
    "\n",
    "                                                        Male or Female?\n",
    "                                                          /           \\\n",
    "                                                         /             \\\n",
    "                                                Ticket Class?           SURVIVED!\n",
    "                                                /     |     \\\n",
    "                                            1st/   2nd|      \\3rd\n",
    "                                              /       |       \\\n",
    "                                        Survived     Died    Died\n",
    "                                        \n",
    "At the end of this notebook, we'll talk more deeply about the specifics of Decision Trees, and the benefits and drawbacks of this particular model.  For now, we'll start by importing the Titanic Dataset, cleaning it, and using a Decision Tree Classifier from the `SKLearn` library to make predictions on the data set!\n",
    "\n",
    "\n",
    "<center><h3>Preparing the Data Set</h3></center>\n",
    "\n",
    "We'll start by reading the dataset in using the `pandas` library.  After that, we'll take the following steps to prepare our dataset for use in a `DecisionTreeClassifier` from SKLearn.\n",
    "\n",
    "**_TASK:_** Read in the `titanic.csv` dataset from the `/datasets` folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to import the needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we need to 'clean' the dataset to get it ready for a Decision Tree.  This steps consists of:\n",
    "\n",
    "1. Storing the 'Survived' column in a separate variable, and removing it from our Decision Tree.  This is what we are trying to predict, which means that we can't leave it in the dataset.  If we did, that would be like giving students the answer key along with the test--everyone would score 100% by default because of this column, and no actual learning would occur. \n",
    "<br>\n",
    "<br>\n",
    "2. Remove unecessary columns such as name, ticket number, etc.  Categorical features are fine, but features like name and ticket number provide no actual information, and will just confuse our algorithm.  Think about which columns should stay, and which should be dropped--in the cell below, we've provided a list of columns to drop for you. \n",
    "<br>\n",
    "<br>\n",
    "3.  Deal with NaN (null values).  In this case, we will deal with them just by dropping the entire row.  There are many other ways of dealing with NaNs, but they are outside the scope of this lesson.  \n",
    "<br>\n",
    "4. Convert categorical values to dummy columns (we've written this code for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop columns that are not helpful\n",
    "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "df_with_cols_dropped = df.drop(columns_to_drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use .dropna() to remove any rows that contain a null value\n",
    "clean_df = df_with_cols_dropped.dropna()\n",
    "\n",
    "# Use pd.get_dummies() on the clean_df object to create dummy columns for categorical variables.  Give the dummy\n",
    "# columns the prefix 'is_' to make it clear that they are dummy varibles. \n",
    "clean_df = pd.get_dummies(clean_df, prefix=['is_', 'is_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the 'Survived' column in the 'labels' variable\n",
    "\n",
    "labels = clean_df['Survived']\n",
    "\n",
    "# Uncomment and run the code below to drop the 'Survived' column from the clean_df dataframe.  \n",
    "clean_df.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "# Ignore the warning pandas gives you when you run this cell--everything is fine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Recap: What We've Done So Far</h3></center>\n",
    "\n",
    "We've now gone through the steps to 'clean' our dataset, and we're almost ready to begin using our Decision Tree Classifier. Let's look at what we've done so far.  \n",
    "\n",
    "1.  Dropped the non-numeric columns.  We can safely assume that no useful information is contained in text-based columns such as name, cabin, etc, so we've dropped them.  \n",
    "2.  Removed rows containing null values. \n",
    "3.  Used. `pd.get_dummies()` to convert categorical columns such as 'Sex' and 'Embarked' into columns for each category, with the category for the actual value containing a '1' while all other dummy columns for that category for that column will contain a '0'.\n",
    "4.  Stored the labels in a separate data set.  It is important that we do this last.  Otherwise, any changes that we make to the corresponding rows in the dataframe will not be reflected in our labels variable.  For instance, if we stored labels in a separate variable and _then_ dropped rows from the dataframe that contain null values, we would still have those corresponding labels for the passengers we've dropped from the dataframe! Separating out our labels only works if the labels for each correspond to the row at the same index in the dataframe.\n",
    "\n",
    "Here's what we'll do next: \n",
    "\n",
    "1. Import the appropriate tools from the `sklearn` library.  We need `train_test_split` to split our data set into training and testing sets,  the actual `DecisionTreeClassifier` model, and `f1_score` to tell us how we did.  \n",
    "<br>\n",
    "2.  Split our data into a **_Training Set_** and a **_Testing Set_**.  Our model will 'learn' on the training set, but we don't want to use this as a metric to tell us how well our model actually does.  In order to see how well our model actually 'learned' how to make predictions, we'll split off a portion of our data to hold out as a testing set.  Our model will make predictions on this data, which gives us a better signal as to whether our model is **_overfitting_**. We'll talk about the concept of **_overfitting_** and **_underfitting_** in a later class.  \n",
    "<br> \n",
    "3.  We'll create a `DecisionTreeClassifier()` object.  \n",
    "<br> \n",
    "4.  We'll `fit()` our model by feeding in our `X_Train` and `y_train` variables.  \n",
    "<br>\n",
    "5.  We'll ask our classifier object to make predictions for every object in the `X_test` variable.   \n",
    "<br>\n",
    "6.  We'll use the `f1_score` object to tell us how well our model made predictions by giving it the predictions made in the last step, as well as the actual labels for those data points, which are currently stored in `y_test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50769230769230766"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, labels)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "# Call clf.fit(), and pass in X_train and y_train as parameters\n",
    "\n",
    "test_pred = clf.predict(X_test) # Use the .predict() method to have our model create predictions on the X_test variable\n",
    "\n",
    "# Finally, pass in test_pred and y_test to the f1_score() object to get an f1_score!\n",
    "f1_score(test_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Branching Out on Your Own</h3></center>\n",
    "\n",
    "Congratulations! You've now successfully used your first Machine Learning algorithm. VC's will now shower you with money any time you say \"Machine Learning\" out loud.\n",
    "\n",
    "<center><img src='img/make_it_rain.gif'></center>\n",
    "\n",
    "<center>Doing Supervised Learning, Unsupervised</center>\n",
    "\n",
    "For the final challenge in this notebook, you'll use a decision tree classifier to make predictions on the Iris Dataset.  The process will be entirely the same as we did on the Titanic dataset, but easier because it requires less data cleaning.  (Hint: Take a look at the [sklearn.datasets](http://scikit-learn.org/stable/datasets/index.html) page to see if you can save yourself some time!)\n",
    "\n",
    "Use a _Decision Tree Classifier_ to try and predict the class of each flower in the Iris dataset.  Use the same methodology you used for the Titanic dataset to get things up and running.\n",
    "\n",
    "**_BONUS:_** See if you can **_visualize_** the Decision Tree you've created!  You can find example code on how to do this fairly easily on the [sklearn's documentation page for Decision Trees](http://scikit-learn.org/stable/modules/tree.html#tree).  Here's an example of visualization from the page:\n",
    "\n",
    "<center><img src='http://scikit-learn.org/stable/_images/iris.svg' height=75% width=75%></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Iris dataset from sklearn and use a decision tree classifier below!\n",
    "import sklearn.datasets as skl\n",
    "skl.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
